{
 "metadata": {
  "name": "",
  "signature": "sha256:63c921cf859ccd2b00b158cd0b42f21684c04ce9a148993430a8002942f2b818"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#DCT Analysis\n",
      "\n",
      "Last week we dealt with the KDD 1998 dataset which concerend donations to a charity.  The data set has a lot of non numeric features which require complicated coding as you saw.  This week we use an anonymized purely numeric dataset so we can skip the complications of real data and move onto modeling.\n",
      "\n",
      "The data is sourced from http://mlcomp.org/datasets/1571 and concerns identifying the gender (an anonymized binary flag -1/1) of people based on 800 anonymized numeric features.  Accoridng the the MLComp website, error rates of ~10% were found using linear models.  We hope to do better using some simple feature selection and feature engineering.\n",
      "\n",
      "We have provided a tab delimited version of the test/train data set in the data directory for your use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import the libraries \n",
      "%pylab inline\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the train and test data into two data frames (`train_data` and `test_data`). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data = pd.read_csv(\"data/train\",sep=\"\\t\")\n",
      "test_data  = pd.read_csv(\"data/test\",sep=\"\\t\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Separate the `label` column from `train_data` into two new data frames called `train_x` and `train_y`. `train_y` is to contain only the column titled `label` in `train_data` while `train_x` should contain all of the rest. `train_x` should contains 800 columns, check that this is the case.\n",
      "\n",
      "In a similar way partition `test_data` into two new data frames called `test_x` and `test_y`\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c_n= len(train_data.columns)\n",
      "train_y= train_data.iloc[:, 0:1].copy().astype(np.float64)\n",
      "train_x= train_data.iloc[:, 1:c_n].copy().astype(np.float64)\n",
      "assert len(train_x.columns) == 800\n",
      "\n",
      "c_n= len(test_data.columns)\n",
      "test_y= test_data.iloc[:, 0:1].copy().astype(np.float64)\n",
      "test_x= test_data.iloc[:, 1:c_n].copy().astype(np.float64)\n",
      "assert len(test_x.columns) == 800"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Summarizing the Data\n",
      "Use pandas to describe the `train_x` data frame.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_x.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>F_0</th>\n",
        "      <th>F_1</th>\n",
        "      <th>F_2</th>\n",
        "      <th>F_3</th>\n",
        "      <th>F_4</th>\n",
        "      <th>F_5</th>\n",
        "      <th>F_6</th>\n",
        "      <th>F_7</th>\n",
        "      <th>F_8</th>\n",
        "      <th>F_9</th>\n",
        "      <th>...</th>\n",
        "      <th>F_790</th>\n",
        "      <th>F_791</th>\n",
        "      <th>F_792</th>\n",
        "      <th>F_793</th>\n",
        "      <th>F_794</th>\n",
        "      <th>F_795</th>\n",
        "      <th>F_796</th>\n",
        "      <th>F_797</th>\n",
        "      <th>F_798</th>\n",
        "      <th>F_799</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td> -85.409660</td>\n",
        "      <td>   5.325581</td>\n",
        "      <td>   1.905188</td>\n",
        "      <td>   0.502683</td>\n",
        "      <td>   1.611807</td>\n",
        "      <td>   0.316637</td>\n",
        "      <td>  -0.064401</td>\n",
        "      <td>  -0.050089</td>\n",
        "      <td>   0.429338</td>\n",
        "      <td>  -0.032200</td>\n",
        "      <td>...</td>\n",
        "      <td>   6.978533</td>\n",
        "      <td>   2.423971</td>\n",
        "      <td>   4.769231</td>\n",
        "      <td>   1.019678</td>\n",
        "      <td>  -0.674419</td>\n",
        "      <td>  -4.341682</td>\n",
        "      <td> -20.626118</td>\n",
        "      <td>  -2.878354</td>\n",
        "      <td>   4.332737</td>\n",
        "      <td>  -0.135957</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>  58.830922</td>\n",
        "      <td>  16.511576</td>\n",
        "      <td>   7.158970</td>\n",
        "      <td>   2.981949</td>\n",
        "      <td>   6.181943</td>\n",
        "      <td>   4.716507</td>\n",
        "      <td>   4.097311</td>\n",
        "      <td>   2.891279</td>\n",
        "      <td>   2.600992</td>\n",
        "      <td>   2.107939</td>\n",
        "      <td>...</td>\n",
        "      <td>  97.125051</td>\n",
        "      <td>  89.087333</td>\n",
        "      <td> 103.323683</td>\n",
        "      <td> 129.015133</td>\n",
        "      <td> 113.575980</td>\n",
        "      <td>  92.834494</td>\n",
        "      <td>  78.890002</td>\n",
        "      <td>  72.975075</td>\n",
        "      <td>  82.473457</td>\n",
        "      <td>  89.108263</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>-157.000000</td>\n",
        "      <td> -40.000000</td>\n",
        "      <td> -31.000000</td>\n",
        "      <td> -23.000000</td>\n",
        "      <td> -14.000000</td>\n",
        "      <td> -28.000000</td>\n",
        "      <td> -25.000000</td>\n",
        "      <td> -15.000000</td>\n",
        "      <td>  -9.000000</td>\n",
        "      <td> -10.000000</td>\n",
        "      <td>...</td>\n",
        "      <td>-287.000000</td>\n",
        "      <td>-279.000000</td>\n",
        "      <td>-368.000000</td>\n",
        "      <td>-375.000000</td>\n",
        "      <td>-313.000000</td>\n",
        "      <td>-323.000000</td>\n",
        "      <td>-284.000000</td>\n",
        "      <td>-277.000000</td>\n",
        "      <td>-314.000000</td>\n",
        "      <td>-237.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>-120.500000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>  -1.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> -60.500000</td>\n",
        "      <td> -54.500000</td>\n",
        "      <td> -62.500000</td>\n",
        "      <td> -88.000000</td>\n",
        "      <td> -72.000000</td>\n",
        "      <td> -62.500000</td>\n",
        "      <td> -71.000000</td>\n",
        "      <td> -47.500000</td>\n",
        "      <td> -49.500000</td>\n",
        "      <td> -58.500000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>-101.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>...</td>\n",
        "      <td>  10.000000</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   7.000000</td>\n",
        "      <td>   3.000000</td>\n",
        "      <td>  -5.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td> -16.000000</td>\n",
        "      <td>  -5.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>  -2.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> -73.500000</td>\n",
        "      <td>   5.000000</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   3.000000</td>\n",
        "      <td>   2.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>...</td>\n",
        "      <td>  77.000000</td>\n",
        "      <td>  56.000000</td>\n",
        "      <td>  75.000000</td>\n",
        "      <td>  90.500000</td>\n",
        "      <td>  75.000000</td>\n",
        "      <td>  59.000000</td>\n",
        "      <td>  30.500000</td>\n",
        "      <td>  43.000000</td>\n",
        "      <td>  61.000000</td>\n",
        "      <td>  57.500000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 158.000000</td>\n",
        "      <td>  80.000000</td>\n",
        "      <td>  42.000000</td>\n",
        "      <td>  17.000000</td>\n",
        "      <td>  49.000000</td>\n",
        "      <td>  25.000000</td>\n",
        "      <td>  18.000000</td>\n",
        "      <td>  15.000000</td>\n",
        "      <td>  29.000000</td>\n",
        "      <td>  12.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> 299.000000</td>\n",
        "      <td> 272.000000</td>\n",
        "      <td> 297.000000</td>\n",
        "      <td> 373.000000</td>\n",
        "      <td> 458.000000</td>\n",
        "      <td> 284.000000</td>\n",
        "      <td> 257.000000</td>\n",
        "      <td> 247.000000</td>\n",
        "      <td> 293.000000</td>\n",
        "      <td> 274.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 800 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "              F_0         F_1         F_2         F_3         F_4         F_5  \\\n",
        "count  559.000000  559.000000  559.000000  559.000000  559.000000  559.000000   \n",
        "mean   -85.409660    5.325581    1.905188    0.502683    1.611807    0.316637   \n",
        "std     58.830922   16.511576    7.158970    2.981949    6.181943    4.716507   \n",
        "min   -157.000000  -40.000000  -31.000000  -23.000000  -14.000000  -28.000000   \n",
        "25%   -120.500000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
        "50%   -101.000000    1.000000    1.000000    0.000000    0.000000    0.000000   \n",
        "75%    -73.500000    5.000000    2.000000    1.000000    3.000000    2.000000   \n",
        "max    158.000000   80.000000   42.000000   17.000000   49.000000   25.000000   \n",
        "\n",
        "              F_6         F_7         F_8         F_9     ...           F_790  \\\n",
        "count  559.000000  559.000000  559.000000  559.000000     ...      559.000000   \n",
        "mean    -0.064401   -0.050089    0.429338   -0.032200     ...        6.978533   \n",
        "std      4.097311    2.891279    2.600992    2.107939     ...       97.125051   \n",
        "min    -25.000000  -15.000000   -9.000000  -10.000000     ...     -287.000000   \n",
        "25%     -1.000000   -1.000000   -1.000000   -1.000000     ...      -60.500000   \n",
        "50%      0.000000    0.000000    0.000000    0.000000     ...       10.000000   \n",
        "75%      1.000000    1.000000    1.000000    1.000000     ...       77.000000   \n",
        "max     18.000000   15.000000   29.000000   12.000000     ...      299.000000   \n",
        "\n",
        "            F_791       F_792       F_793       F_794       F_795       F_796  \\\n",
        "count  559.000000  559.000000  559.000000  559.000000  559.000000  559.000000   \n",
        "mean     2.423971    4.769231    1.019678   -0.674419   -4.341682  -20.626118   \n",
        "std     89.087333  103.323683  129.015133  113.575980   92.834494   78.890002   \n",
        "min   -279.000000 -368.000000 -375.000000 -313.000000 -323.000000 -284.000000   \n",
        "25%    -54.500000  -62.500000  -88.000000  -72.000000  -62.500000  -71.000000   \n",
        "50%      0.000000    7.000000    3.000000   -5.000000    1.000000  -16.000000   \n",
        "75%     56.000000   75.000000   90.500000   75.000000   59.000000   30.500000   \n",
        "max    272.000000  297.000000  373.000000  458.000000  284.000000  257.000000   \n",
        "\n",
        "            F_797       F_798       F_799  \n",
        "count  559.000000  559.000000  559.000000  \n",
        "mean    -2.878354    4.332737   -0.135957  \n",
        "std     72.975075   82.473457   89.108263  \n",
        "min   -277.000000 -314.000000 -237.000000  \n",
        "25%    -47.500000  -49.500000  -58.500000  \n",
        "50%     -5.000000    1.000000   -2.000000  \n",
        "75%     43.000000   61.000000   57.500000  \n",
        "max    247.000000  293.000000  274.000000  \n",
        "\n",
        "[8 rows x 800 columns]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_x.info()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 239 entries, 0 to 238\n",
        "Columns: 800 entries, F_0 to F_799\n",
        "dtypes: float64(800)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the range of each column varies by a significant amount.  Use the Z-Scaling method to normalize the data and make it suitable to modeling. More specifically, for each column in `train_x` compute the average $\\mu$ and the standard deviation $\\sigma$. Then transform each value in the column from the original value $X$ to the Z-value $Z$ using the formula:\n",
      "\n",
      "$$ Z=\\frac{X-\\mu}{\\sigma} $$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "\n",
      "# create logger\n",
      "log = logging.getLogger('df_zscore_logger')\n",
      "log.setLevel(logging.WARN)\n",
      "\n",
      "def zscore(df, m_mean=None, m_std=None):\n",
      "    if m_mean is None or m_std is None:\n",
      "        m_mean= df.mean()\n",
      "        m_std= df.std()\n",
      "    \n",
      "    m_columns= new_train_x.columns\n",
      "    #m_columns= [\"F_1\"]\n",
      "    for c in m_columns:\n",
      "        c_mean= m_mean[c]\n",
      "        c_std= m_std[c]\n",
      "        log.debug(\"c_mean={}; c_std={}\".format(c_mean, c_std))\n",
      "        log.debug(\"Before --> df[c].head= {}\".format(df[c].head()))\n",
      "        log.debug(\"Before --> df[c].describe= {}\".format(df[c].describe()))\n",
      "        df[c] = (df[c] - c_mean) / c_std\n",
      "        log.debug(\"After --> df[c].head= {}\".format(df[c].head()))\n",
      "        log.debug(\"After --> df[c].describe= {}\".format(df[c].describe()))\n",
      "    \n",
      "    return (m_mean, m_std)\n",
      "        \n",
      "\n",
      "new_train_x= train_x.copy()\n",
      "train_x_mean, train_x_std= zscore(new_train_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_train_x.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>F_0</th>\n",
        "      <th>F_1</th>\n",
        "      <th>F_2</th>\n",
        "      <th>F_3</th>\n",
        "      <th>F_4</th>\n",
        "      <th>F_5</th>\n",
        "      <th>F_6</th>\n",
        "      <th>F_7</th>\n",
        "      <th>F_8</th>\n",
        "      <th>F_9</th>\n",
        "      <th>...</th>\n",
        "      <th>F_790</th>\n",
        "      <th>F_791</th>\n",
        "      <th>F_792</th>\n",
        "      <th>F_793</th>\n",
        "      <th>F_794</th>\n",
        "      <th>F_795</th>\n",
        "      <th>F_796</th>\n",
        "      <th>F_797</th>\n",
        "      <th>F_798</th>\n",
        "      <th>F_799</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td>...</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td> 8.262125e-17</td>\n",
        "      <td> 3.495514e-17</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>-1.271096e-17</td>\n",
        "      <td> 1.588870e-18</td>\n",
        "      <td> 1.271096e-17</td>\n",
        "      <td> 2.383305e-17</td>\n",
        "      <td> 1.271096e-17</td>\n",
        "      <td> 5.561046e-18</td>\n",
        "      <td> 6.355481e-18</td>\n",
        "      <td>...</td>\n",
        "      <td>-3.177740e-17</td>\n",
        "      <td> 2.065531e-17</td>\n",
        "      <td>-2.542192e-17</td>\n",
        "      <td>-2.542192e-17</td>\n",
        "      <td>-1.271096e-17</td>\n",
        "      <td> 1.271096e-17</td>\n",
        "      <td> 3.813288e-17</td>\n",
        "      <td> 1.271096e-17</td>\n",
        "      <td>-6.355481e-18</td>\n",
        "      <td> 7.944351e-19</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td>...</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>-1.216883e+00</td>\n",
        "      <td>-2.745079e+00</td>\n",
        "      <td>  -4.596358</td>\n",
        "      <td>-7.881653e+00</td>\n",
        "      <td>-2.525388e+00</td>\n",
        "      <td>-6.003730e+00</td>\n",
        "      <td>-6.085844e+00</td>\n",
        "      <td>-5.170691e+00</td>\n",
        "      <td>-3.625285e+00</td>\n",
        "      <td>-4.728695e+00</td>\n",
        "      <td>...</td>\n",
        "      <td>-3.026804e+00</td>\n",
        "      <td>-3.158967e+00</td>\n",
        "      <td>-3.607781e+00</td>\n",
        "      <td>-2.914539e+00</td>\n",
        "      <td>-2.749926e+00</td>\n",
        "      <td>-3.432542e+00</td>\n",
        "      <td>-3.338495e+00</td>\n",
        "      <td>-3.756374e+00</td>\n",
        "      <td>-3.859820e+00</td>\n",
        "      <td>-2.658160e+00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>-5.964608e-01</td>\n",
        "      <td>-3.830998e-01</td>\n",
        "      <td>  -0.405811</td>\n",
        "      <td>-5.039266e-01</td>\n",
        "      <td>-4.224896e-01</td>\n",
        "      <td>-2.791551e-01</td>\n",
        "      <td>-2.283447e-01</td>\n",
        "      <td>-3.285434e-01</td>\n",
        "      <td>-5.495358e-01</td>\n",
        "      <td>-4.591213e-01</td>\n",
        "      <td>...</td>\n",
        "      <td>-6.947593e-01</td>\n",
        "      <td>-6.389682e-01</td>\n",
        "      <td>-6.510534e-01</td>\n",
        "      <td>-6.899941e-01</td>\n",
        "      <td>-6.279988e-01</td>\n",
        "      <td>-6.264732e-01</td>\n",
        "      <td>-6.385332e-01</td>\n",
        "      <td>-6.114642e-01</td>\n",
        "      <td>-6.527280e-01</td>\n",
        "      <td>-6.549790e-01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>-2.650025e-01</td>\n",
        "      <td>-2.619726e-01</td>\n",
        "      <td>  -0.126441</td>\n",
        "      <td>-1.685755e-01</td>\n",
        "      <td>-2.607282e-01</td>\n",
        "      <td>-6.713376e-02</td>\n",
        "      <td> 1.571780e-02</td>\n",
        "      <td> 1.732432e-02</td>\n",
        "      <td>-1.650671e-01</td>\n",
        "      <td> 1.527576e-02</td>\n",
        "      <td>...</td>\n",
        "      <td> 3.110904e-02</td>\n",
        "      <td>-2.720893e-02</td>\n",
        "      <td> 2.159011e-02</td>\n",
        "      <td> 1.534953e-02</td>\n",
        "      <td>-3.808535e-02</td>\n",
        "      <td> 5.753984e-02</td>\n",
        "      <td> 5.864011e-02</td>\n",
        "      <td>-2.907357e-02</td>\n",
        "      <td>-4.040981e-02</td>\n",
        "      <td>-2.091886e-02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> 2.024388e-01</td>\n",
        "      <td>-1.971837e-02</td>\n",
        "      <td>   0.013244</td>\n",
        "      <td> 1.667757e-01</td>\n",
        "      <td> 2.245561e-01</td>\n",
        "      <td> 3.569089e-01</td>\n",
        "      <td> 2.597803e-01</td>\n",
        "      <td> 3.631920e-01</td>\n",
        "      <td> 2.194016e-01</td>\n",
        "      <td> 4.896728e-01</td>\n",
        "      <td>...</td>\n",
        "      <td> 7.209414e-01</td>\n",
        "      <td> 6.013877e-01</td>\n",
        "      <td> 6.797161e-01</td>\n",
        "      <td> 6.935645e-01</td>\n",
        "      <td> 6.662889e-01</td>\n",
        "      <td> 6.823076e-01</td>\n",
        "      <td> 6.480684e-01</td>\n",
        "      <td> 6.286853e-01</td>\n",
        "      <td> 6.870970e-01</td>\n",
        "      <td> 6.468082e-01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 4.137444e+00</td>\n",
        "      <td> 4.522549e+00</td>\n",
        "      <td>   5.600640</td>\n",
        "      <td> 5.532395e+00</td>\n",
        "      <td> 7.665582e+00</td>\n",
        "      <td> 5.233399e+00</td>\n",
        "      <td> 4.408843e+00</td>\n",
        "      <td> 5.205340e+00</td>\n",
        "      <td> 1.098453e+01</td>\n",
        "      <td> 5.708041e+00</td>\n",
        "      <td>...</td>\n",
        "      <td> 3.006654e+00</td>\n",
        "      <td> 3.025975e+00</td>\n",
        "      <td> 2.828304e+00</td>\n",
        "      <td> 2.883230e+00</td>\n",
        "      <td> 4.038481e+00</td>\n",
        "      <td> 3.105976e+00</td>\n",
        "      <td> 3.519155e+00</td>\n",
        "      <td> 3.424160e+00</td>\n",
        "      <td> 3.500123e+00</td>\n",
        "      <td> 3.076437e+00</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 800 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "                F_0           F_1         F_2           F_3           F_4  \\\n",
        "count  5.590000e+02  5.590000e+02  559.000000  5.590000e+02  5.590000e+02   \n",
        "mean   8.262125e-17  3.495514e-17    0.000000 -1.271096e-17  1.588870e-18   \n",
        "std    1.000000e+00  1.000000e+00    1.000000  1.000000e+00  1.000000e+00   \n",
        "min   -1.216883e+00 -2.745079e+00   -4.596358 -7.881653e+00 -2.525388e+00   \n",
        "25%   -5.964608e-01 -3.830998e-01   -0.405811 -5.039266e-01 -4.224896e-01   \n",
        "50%   -2.650025e-01 -2.619726e-01   -0.126441 -1.685755e-01 -2.607282e-01   \n",
        "75%    2.024388e-01 -1.971837e-02    0.013244  1.667757e-01  2.245561e-01   \n",
        "max    4.137444e+00  4.522549e+00    5.600640  5.532395e+00  7.665582e+00   \n",
        "\n",
        "                F_5           F_6           F_7           F_8           F_9  \\\n",
        "count  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02   \n",
        "mean   1.271096e-17  2.383305e-17  1.271096e-17  5.561046e-18  6.355481e-18   \n",
        "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
        "min   -6.003730e+00 -6.085844e+00 -5.170691e+00 -3.625285e+00 -4.728695e+00   \n",
        "25%   -2.791551e-01 -2.283447e-01 -3.285434e-01 -5.495358e-01 -4.591213e-01   \n",
        "50%   -6.713376e-02  1.571780e-02  1.732432e-02 -1.650671e-01  1.527576e-02   \n",
        "75%    3.569089e-01  2.597803e-01  3.631920e-01  2.194016e-01  4.896728e-01   \n",
        "max    5.233399e+00  4.408843e+00  5.205340e+00  1.098453e+01  5.708041e+00   \n",
        "\n",
        "           ...              F_790         F_791         F_792         F_793  \\\n",
        "count      ...       5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02   \n",
        "mean       ...      -3.177740e-17  2.065531e-17 -2.542192e-17 -2.542192e-17   \n",
        "std        ...       1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
        "min        ...      -3.026804e+00 -3.158967e+00 -3.607781e+00 -2.914539e+00   \n",
        "25%        ...      -6.947593e-01 -6.389682e-01 -6.510534e-01 -6.899941e-01   \n",
        "50%        ...       3.110904e-02 -2.720893e-02  2.159011e-02  1.534953e-02   \n",
        "75%        ...       7.209414e-01  6.013877e-01  6.797161e-01  6.935645e-01   \n",
        "max        ...       3.006654e+00  3.025975e+00  2.828304e+00  2.883230e+00   \n",
        "\n",
        "              F_794         F_795         F_796         F_797         F_798  \\\n",
        "count  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02   \n",
        "mean  -1.271096e-17  1.271096e-17  3.813288e-17  1.271096e-17 -6.355481e-18   \n",
        "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
        "min   -2.749926e+00 -3.432542e+00 -3.338495e+00 -3.756374e+00 -3.859820e+00   \n",
        "25%   -6.279988e-01 -6.264732e-01 -6.385332e-01 -6.114642e-01 -6.527280e-01   \n",
        "50%   -3.808535e-02  5.753984e-02  5.864011e-02 -2.907357e-02 -4.040981e-02   \n",
        "75%    6.662889e-01  6.823076e-01  6.480684e-01  6.286853e-01  6.870970e-01   \n",
        "max    4.038481e+00  3.105976e+00  3.519155e+00  3.424160e+00  3.500123e+00   \n",
        "\n",
        "              F_799  \n",
        "count  5.590000e+02  \n",
        "mean   7.944351e-19  \n",
        "std    1.000000e+00  \n",
        "min   -2.658160e+00  \n",
        "25%   -6.549790e-01  \n",
        "50%   -2.091886e-02  \n",
        "75%    6.468082e-01  \n",
        "max    3.076437e+00  \n",
        "\n",
        "[8 rows x 800 columns]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the values of $\\mu$ and $\\sigma$ computed from `train_x` to scale the values in `test_x`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "\n",
      "# create logger\n",
      "log = logging.getLogger('df_zscore_testx_logger')\n",
      "log.setLevel(logging.WARN)\n",
      "\n",
      "new_test_x= test_x.copy()\n",
      "zscore(new_test_x, m_mean=train_x_mean, m_std=train_x_std)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "(F_0    -85.409660\n",
        " F_1      5.325581\n",
        " F_2      1.905188\n",
        " F_3      0.502683\n",
        " F_4      1.611807\n",
        " F_5      0.316637\n",
        " F_6     -0.064401\n",
        " F_7     -0.050089\n",
        " F_8      0.429338\n",
        " F_9     -0.032200\n",
        " F_10     0.030411\n",
        " F_11     0.023256\n",
        " F_12     0.101968\n",
        " F_13    -0.008945\n",
        " F_14    -0.044723\n",
        " ...\n",
        " F_785    -7.116279\n",
        " F_786   -16.041145\n",
        " F_787   -12.942755\n",
        " F_788   -19.472272\n",
        " F_789    -5.325581\n",
        " F_790     6.978533\n",
        " F_791     2.423971\n",
        " F_792     4.769231\n",
        " F_793     1.019678\n",
        " F_794    -0.674419\n",
        " F_795    -4.341682\n",
        " F_796   -20.626118\n",
        " F_797    -2.878354\n",
        " F_798     4.332737\n",
        " F_799    -0.135957\n",
        " Length: 800, dtype: float64, F_0     58.830922\n",
        " F_1     16.511576\n",
        " F_2      7.158970\n",
        " F_3      2.981949\n",
        " F_4      6.181943\n",
        " F_5      4.716507\n",
        " F_6      4.097311\n",
        " F_7      2.891279\n",
        " F_8      2.600992\n",
        " F_9      2.107939\n",
        " F_10     1.946637\n",
        " F_11     1.767233\n",
        " F_12     1.436336\n",
        " F_13     1.310938\n",
        " F_14     1.406514\n",
        " ...\n",
        " F_785    102.375315\n",
        " F_786     86.126023\n",
        " F_787    107.377205\n",
        " F_788    136.200222\n",
        " F_789    125.066755\n",
        " F_790     97.125051\n",
        " F_791     89.087333\n",
        " F_792    103.323683\n",
        " F_793    129.015133\n",
        " F_794    113.575980\n",
        " F_795     92.834494\n",
        " F_796     78.890002\n",
        " F_797     72.975075\n",
        " F_798     82.473457\n",
        " F_799     89.108263\n",
        " Length: 800, dtype: float64)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_test_x.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>F_0</th>\n",
        "      <th>F_1</th>\n",
        "      <th>F_2</th>\n",
        "      <th>F_3</th>\n",
        "      <th>F_4</th>\n",
        "      <th>F_5</th>\n",
        "      <th>F_6</th>\n",
        "      <th>F_7</th>\n",
        "      <th>F_8</th>\n",
        "      <th>F_9</th>\n",
        "      <th>...</th>\n",
        "      <th>F_790</th>\n",
        "      <th>F_791</th>\n",
        "      <th>F_792</th>\n",
        "      <th>F_793</th>\n",
        "      <th>F_794</th>\n",
        "      <th>F_795</th>\n",
        "      <th>F_796</th>\n",
        "      <th>F_797</th>\n",
        "      <th>F_798</th>\n",
        "      <th>F_799</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>  -0.048084</td>\n",
        "      <td>   0.077335</td>\n",
        "      <td>   0.020257</td>\n",
        "      <td>   0.072765</td>\n",
        "      <td>   0.021508</td>\n",
        "      <td>  -0.053827</td>\n",
        "      <td>   0.139281</td>\n",
        "      <td>  -0.043456</td>\n",
        "      <td>  -0.033157</td>\n",
        "      <td>   0.011306</td>\n",
        "      <td>...</td>\n",
        "      <td>  -0.018519</td>\n",
        "      <td>   0.013229</td>\n",
        "      <td>   0.078567</td>\n",
        "      <td>   0.026311</td>\n",
        "      <td>  -0.057758</td>\n",
        "      <td>  -0.149649</td>\n",
        "      <td>  -0.011793</td>\n",
        "      <td>   0.047642</td>\n",
        "      <td>   0.103214</td>\n",
        "      <td>   0.064352</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>   0.922365</td>\n",
        "      <td>   0.932226</td>\n",
        "      <td>   0.982286</td>\n",
        "      <td>   1.007044</td>\n",
        "      <td>   1.061437</td>\n",
        "      <td>   1.143097</td>\n",
        "      <td>   1.196676</td>\n",
        "      <td>   1.101199</td>\n",
        "      <td>   0.952202</td>\n",
        "      <td>   1.244538</td>\n",
        "      <td>...</td>\n",
        "      <td>   1.082358</td>\n",
        "      <td>   1.052719</td>\n",
        "      <td>   1.043850</td>\n",
        "      <td>   1.052912</td>\n",
        "      <td>   1.096577</td>\n",
        "      <td>   0.964285</td>\n",
        "      <td>   0.996227</td>\n",
        "      <td>   1.051661</td>\n",
        "      <td>   0.913283</td>\n",
        "      <td>   0.939877</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>  -1.080900</td>\n",
        "      <td>  -2.866206</td>\n",
        "      <td>  -3.199509</td>\n",
        "      <td>  -4.863492</td>\n",
        "      <td>  -4.143003</td>\n",
        "      <td>  -7.063837</td>\n",
        "      <td>  -5.109595</td>\n",
        "      <td>  -6.554162</td>\n",
        "      <td>  -4.394223</td>\n",
        "      <td>  -5.677489</td>\n",
        "      <td>...</td>\n",
        "      <td>  -3.253317</td>\n",
        "      <td>  -3.069168</td>\n",
        "      <td>  -2.668984</td>\n",
        "      <td>  -2.682008</td>\n",
        "      <td>  -3.128528</td>\n",
        "      <td>  -3.367911</td>\n",
        "      <td>  -3.477930</td>\n",
        "      <td>  -2.824549</td>\n",
        "      <td>  -2.829186</td>\n",
        "      <td>  -2.130712</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>  -0.587962</td>\n",
        "      <td>  -0.383100</td>\n",
        "      <td>  -0.266126</td>\n",
        "      <td>  -0.168575</td>\n",
        "      <td>  -0.422490</td>\n",
        "      <td>  -0.491176</td>\n",
        "      <td>  -0.228345</td>\n",
        "      <td>  -0.328543</td>\n",
        "      <td>  -0.549536</td>\n",
        "      <td>  -0.459121</td>\n",
        "      <td>...</td>\n",
        "      <td>  -0.771979</td>\n",
        "      <td>  -0.678255</td>\n",
        "      <td>  -0.525235</td>\n",
        "      <td>  -0.744251</td>\n",
        "      <td>  -0.694034</td>\n",
        "      <td>  -0.755735</td>\n",
        "      <td>  -0.537126</td>\n",
        "      <td>  -0.611464</td>\n",
        "      <td>  -0.507227</td>\n",
        "      <td>  -0.553978</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>  -0.366990</td>\n",
        "      <td>  -0.201409</td>\n",
        "      <td>  -0.126441</td>\n",
        "      <td>  -0.168575</td>\n",
        "      <td>  -0.098967</td>\n",
        "      <td>  -0.067134</td>\n",
        "      <td>   0.015718</td>\n",
        "      <td>   0.017324</td>\n",
        "      <td>  -0.165067</td>\n",
        "      <td>   0.015276</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.031109</td>\n",
        "      <td>   0.017691</td>\n",
        "      <td>   0.021590</td>\n",
        "      <td>   0.069607</td>\n",
        "      <td>  -0.038085</td>\n",
        "      <td>  -0.114810</td>\n",
        "      <td>  -0.004739</td>\n",
        "      <td>   0.012036</td>\n",
        "      <td>   0.117217</td>\n",
        "      <td>   0.023970</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>   0.083454</td>\n",
        "      <td>   0.040845</td>\n",
        "      <td>   0.013244</td>\n",
        "      <td>   0.166776</td>\n",
        "      <td>   0.224556</td>\n",
        "      <td>   0.356909</td>\n",
        "      <td>   0.259780</td>\n",
        "      <td>   0.363192</td>\n",
        "      <td>   0.219402</td>\n",
        "      <td>   0.489673</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.720941</td>\n",
        "      <td>   0.629450</td>\n",
        "      <td>   0.699073</td>\n",
        "      <td>   0.794328</td>\n",
        "      <td>   0.697105</td>\n",
        "      <td>   0.380696</td>\n",
        "      <td>   0.591027</td>\n",
        "      <td>   0.731460</td>\n",
        "      <td>   0.662847</td>\n",
        "      <td>   0.674864</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>   3.763491</td>\n",
        "      <td>   3.856350</td>\n",
        "      <td>   4.064106</td>\n",
        "      <td>   3.855639</td>\n",
        "      <td>   5.562684</td>\n",
        "      <td>   4.597335</td>\n",
        "      <td>   7.093530</td>\n",
        "      <td>   6.242943</td>\n",
        "      <td>   3.679620</td>\n",
        "      <td>   8.080026</td>\n",
        "      <td>...</td>\n",
        "      <td>   3.593527</td>\n",
        "      <td>   2.790251</td>\n",
        "      <td>   3.679996</td>\n",
        "      <td>   2.890981</td>\n",
        "      <td>   2.779412</td>\n",
        "      <td>   2.858223</td>\n",
        "      <td>   2.885361</td>\n",
        "      <td>   3.218611</td>\n",
        "      <td>   2.736241</td>\n",
        "      <td>   2.694879</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 800 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "              F_0         F_1         F_2         F_3         F_4         F_5  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000  239.000000  239.000000   \n",
        "mean    -0.048084    0.077335    0.020257    0.072765    0.021508   -0.053827   \n",
        "std      0.922365    0.932226    0.982286    1.007044    1.061437    1.143097   \n",
        "min     -1.080900   -2.866206   -3.199509   -4.863492   -4.143003   -7.063837   \n",
        "25%     -0.587962   -0.383100   -0.266126   -0.168575   -0.422490   -0.491176   \n",
        "50%     -0.366990   -0.201409   -0.126441   -0.168575   -0.098967   -0.067134   \n",
        "75%      0.083454    0.040845    0.013244    0.166776    0.224556    0.356909   \n",
        "max      3.763491    3.856350    4.064106    3.855639    5.562684    4.597335   \n",
        "\n",
        "              F_6         F_7         F_8         F_9     ...           F_790  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000     ...      239.000000   \n",
        "mean     0.139281   -0.043456   -0.033157    0.011306     ...       -0.018519   \n",
        "std      1.196676    1.101199    0.952202    1.244538     ...        1.082358   \n",
        "min     -5.109595   -6.554162   -4.394223   -5.677489     ...       -3.253317   \n",
        "25%     -0.228345   -0.328543   -0.549536   -0.459121     ...       -0.771979   \n",
        "50%      0.015718    0.017324   -0.165067    0.015276     ...        0.031109   \n",
        "75%      0.259780    0.363192    0.219402    0.489673     ...        0.720941   \n",
        "max      7.093530    6.242943    3.679620    8.080026     ...        3.593527   \n",
        "\n",
        "            F_791       F_792       F_793       F_794       F_795       F_796  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000  239.000000  239.000000   \n",
        "mean     0.013229    0.078567    0.026311   -0.057758   -0.149649   -0.011793   \n",
        "std      1.052719    1.043850    1.052912    1.096577    0.964285    0.996227   \n",
        "min     -3.069168   -2.668984   -2.682008   -3.128528   -3.367911   -3.477930   \n",
        "25%     -0.678255   -0.525235   -0.744251   -0.694034   -0.755735   -0.537126   \n",
        "50%      0.017691    0.021590    0.069607   -0.038085   -0.114810   -0.004739   \n",
        "75%      0.629450    0.699073    0.794328    0.697105    0.380696    0.591027   \n",
        "max      2.790251    3.679996    2.890981    2.779412    2.858223    2.885361   \n",
        "\n",
        "            F_797       F_798       F_799  \n",
        "count  239.000000  239.000000  239.000000  \n",
        "mean     0.047642    0.103214    0.064352  \n",
        "std      1.051661    0.913283    0.939877  \n",
        "min     -2.824549   -2.829186   -2.130712  \n",
        "25%     -0.611464   -0.507227   -0.553978  \n",
        "50%      0.012036    0.117217    0.023970  \n",
        "75%      0.731460    0.662847    0.674864  \n",
        "max      3.218611    2.736241    2.694879  \n",
        "\n",
        "[8 rows x 800 columns]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Afterwards summarize the train_x and test_x post processing.  Train_x should have exactly 0 mean and 1 standard deviation while test_x should be nearly there."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "\n",
      "# create logger\n",
      "log = logging.getLogger('summary_logger')\n",
      "log.setLevel(logging.INFO)\n",
      "\n",
      "m_train_x_summ= new_train_x.describe()\n",
      "log.info(\"Summarizing m_train_x_summ\")\n",
      "for k,v in m_train_x_summ.iteritems():\n",
      "    log.info(\"Summary=\\n{}\\n\".format(v))\n",
      "\n",
      "m_test_x_summ= new_test_x.describe()\n",
      "log.info(\"Summarizing m_test_x_summ\")\n",
      "for k,v in m_test_x_summ.iteritems():\n",
      "    log.info(\"Summary=\\n{}\\n\".format(v))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Studying The Target\n",
      "\n",
      "Now that we have normalized the input matrix, we move on to looking at the target.  Within the train_y data frame, what is the percentage of positive examples?  This is our prior probability and the accuracy we will attain if we always guess positive.  Also measure the prior for the test_y frame and comment if there are major differences between the testing/training data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m_total_rows= float(len(train_y))\n",
      "p_mask= train_y[train_y['label'] > 0]\n",
      "m_positive_count= float(len(p_mask))\n",
      "m_prior_percent_positive= round(m_positive_count/m_total_rows, 6)\n",
      "\n",
      "mtest_total_rows= float(len(test_y))\n",
      "ptest_mask= test_y[test_y['label'] > 0]\n",
      "mtest_positive_count= float(len(ptest_mask))\n",
      "mtest_prior_percent_positive= round(mtest_positive_count/mtest_total_rows, 6)\n",
      "\n",
      "print m_positive_count, m_total_rows, m_prior_percent_positive\n",
      "print mtest_positive_count, mtest_total_rows, mtest_prior_percent_positive"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "276.0 559.0 0.493739\n",
        "123.0 239.0 0.514644\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Supervised Modeling\n",
      "\n",
      "We choose to use logistic regression on this data set since it is a simple binary classification problem.  In this section we aim to train a logistic regression model that accurately predicts the class of each record.  We'll train using the train_x/train_y data frames and evaluate on the test_x/test_y data frames.\n",
      "\n",
      "###Baseline Model\n",
      "\n",
      "Use sklearn's linear model package to train a logistic regression model using the whole train_x/train_y data frames.  Then evaluate this model using the test_x/test_y data frames by computing the accuracy (% of correct predictions).  Store this computed accuracy as acc_baseline."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "\n",
      "# create logger\n",
      "log = logging.getLogger('get_accuracy_logger')\n",
      "log.setLevel(logging.DEBUG)\n",
      "\n",
      "def get_accuracy(m_actual_array, m_predicted):\n",
      "    m_total= len(m_actual_array)\n",
      "    m_predicted_count= len(m_predicted)\n",
      "    assert m_predicted_count == m_total\n",
      "    \n",
      "    log.debug(\"m_actual_array: {}, {}\".format(type(m_actual_array), shape(m_actual_array)))\n",
      "    log.debug(\"m_predicted: {}, {}\".format(type(m_predicted), shape(m_predicted)))\n",
      "    m_matching_arr= (m_actual_array == m_predicted).nonzero()[0]\n",
      "    log.debug(m_matching_arr)\n",
      "    \n",
      "    m_result= round(float(len(m_matching_arr)) / float(m_predicted_count), 6)\n",
      "    log.debug(\"m_result={}\".format(m_result))\n",
      "    return m_result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "from numpy.random import RandomState\n",
      "\n",
      "rng = RandomState(45)\n",
      "# #train model and evaluate performance here\n",
      "# C= 0.09\n",
      "# m_l1_LR= LogisticRegression(C=C, penalty='l1', tol=0.000001, random_state=rng)\n",
      "\n",
      "def new_logistic_regression():\n",
      "    return LogisticRegression(random_state=rng)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m_l1_LR= new_logistic_regression()\n",
      "m_l1_LR.fit(new_train_x, train_y[\"label\"])\n",
      "\n",
      "m_predicted= m_l1_LR.predict(new_test_x)\n",
      "m_actual_array= test_y[\"label\"].values\n",
      "\n",
      "acc_baseline= get_accuracy(m_actual_array, m_predicted)\n",
      "print \"acc_baseline= {}\".format(acc_baseline)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:get_accuracy_logger:m_actual_array: <type 'numpy.ndarray'>, (239,)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:get_accuracy_logger:m_predicted: <type 'numpy.ndarray'>, (239,)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:get_accuracy_logger:[  0   1   2   4   5   6   7   9  10  11  12  13  14  15  16  17  18  19\n",
        "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
        "  39  40  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
        "  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75\n",
        "  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  92  93  94\n",
        "  95  96  97  98 100 101 102 103 104 105 106 107 108 109 110 112 113 114\n",
        " 115 116 117 118 119 120 121 122 123 124 125 126 127 129 130 131 132 134\n",
        " 135 136 137 138 139 140 141 142 143 144 145 147 148 150 151 152 153 154\n",
        " 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172\n",
        " 173 174 175 176 177 178 179 180 183 184 185 186 188 189 190 191 193 194\n",
        " 195 196 197 198 199 200 201 202 203 204 205 206 207 209 210 212 213 214\n",
        " 215 216 217 218 219 220 221 223 224 225 226 227 229 230 231 233 234 235\n",
        " 236 237 238]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:get_accuracy_logger:m_result=0.916318\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "acc_baseline= 0.916318\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Simpler model \n",
      "While your previous model has good performance, it uses 800 features.  The high feature count means the model is difficult to explain and may result in overfitting (since there are under 800 records).  We wish to build a model with 10 features which also has good accuracy.\n",
      "\n",
      "As a first step towards this, find the 20 features which have the largest absolute coefficients in your old model and build a logistic regression model which only uses these 20 features.  Then measure the accuracy of this new model and comment on the impact to performance caused by just using these 20 features.\n",
      "\n",
      "To do this you will have to make copies of the train_x/test_x data frames which only have the 20 highest coefficient features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k= 20\n",
      "m_coef_l1_LR = m_l1_LR.coef_.ravel()\n",
      "Coef_List=zip(new_train_x.columns.tolist(),m_coef_l1_LR)\n",
      "Coef_List.sort(key=lambda x: abs(x[1]),reverse=True)\n",
      "\n",
      "m_selected_columns= [Coef_List[i][0] for i in xrange(k)]\n",
      "\n",
      "m_train_x_best= new_train_x[m_selected_columns]\n",
      "m_test_x_best= new_test_x[m_selected_columns]\n",
      "\n",
      "new_m_l1_LR= new_logistic_regression()\n",
      "new_m_l1_LR.fit(m_train_x_best, train_y[\"label\"])\n",
      "\n",
      "m_predicted= new_m_l1_LR.predict(m_test_x_best)\n",
      "m_actual_array= test_y[\"label\"].values\n",
      "\n",
      "acc_with_best= get_accuracy(m_actual_array, m_predicted)\n",
      "print \"acc_with_best= {}\".format(acc_with_best)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:get_accuracy_logger:m_actual_array: <type 'numpy.ndarray'>, (239,)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:get_accuracy_logger:m_predicted: <type 'numpy.ndarray'>, (239,)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:get_accuracy_logger:[  0   1   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
        "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
        "  37  38  39  40  41  42  43  44  45  46  48  49  50  51  52  53  54  56\n",
        "  57  58  59  60  62  63  64  65  66  67  68  69  70  71  72  73  75  77\n",
        "  78  79  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96\n",
        "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 115\n",
        " 116 117 118 119 120 122 123 124 125 126 127 128 129 131 132 133 134 135\n",
        " 136 137 138 139 140 142 143 144 145 147 150 151 152 153 154 155 156 157\n",
        " 158 159 160 161 162 163 165 166 167 168 169 170 171 172 173 174 175 176\n",
        " 177 178 179 180 181 182 183 185 188 189 190 191 192 193 194 195 196 197\n",
        " 198 199 200 201 202 203 204 205 206 209 210 212 213 214 215 217 219 220\n",
        " 221 224 225 226 227 229 230 231 232 233 234 235 236 237 238]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "DEBUG:get_accuracy_logger:m_result=0.891213\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "acc_with_best= 0.891213\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange(60):\n",
      "    print Coef_List[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('F_240', 0.67590888043048003)\n",
        "('F_160', 0.3857004941521569)\n",
        "('F_495', -0.37082746592174975)\n",
        "('F_321', -0.36875530565160275)\n",
        "('F_393', -0.35514906298353194)\n",
        "('F_459', -0.34240157641473956)\n",
        "('F_346', -0.32114124402745009)\n",
        "('F_466', -0.31683219425822462)\n",
        "('F_585', -0.31460169599349386)\n",
        "('F_320', -0.30400555887167002)\n",
        "('F_108', 0.30153033686955716)\n",
        "('F_371', 0.29787563288422547)\n",
        "('F_416', 0.29558205876242699)\n",
        "('F_418', -0.29337412064721269)\n",
        "('F_592', 0.29259636523380006)\n",
        "('F_691', -0.28906351575085837)\n",
        "('F_450', 0.27909121219379324)\n",
        "('F_498', 0.27201141118412431)\n",
        "('F_345', 0.27195769807482473)\n",
        "('F_563', 0.26889247884358786)\n",
        "('F_80', 0.26649639874575903)\n",
        "('F_396', 0.26472161028525476)\n",
        "('F_440', 0.26257298726624456)\n",
        "('F_636', 0.25964090094482717)\n",
        "('F_192', -0.25797832296202078)\n",
        "('F_377', -0.25704225908997452)\n",
        "('F_325', 0.25326833592084069)\n",
        "('F_417', 0.24738517493554765)\n",
        "('F_586', -0.24526293009108532)\n",
        "('F_64', 0.24469887053348535)\n",
        "('F_632', -0.24406549970641825)\n",
        "('F_588', -0.24071825361579294)\n",
        "('F_350', 0.23996368255096157)\n",
        "('F_215', 0.23713646223429258)\n",
        "('F_444', -0.23678357854459142)\n",
        "('F_89', 0.23322930743083786)\n",
        "('F_322', -0.23206917294372004)\n",
        "('F_619', -0.23128760685703301)\n",
        "('F_337', 0.22971780983924958)\n",
        "('F_153', -0.22904442884520598)\n",
        "('F_112', -0.22893262737195016)\n",
        "('F_651', 0.22829651840227214)\n",
        "('F_490', 0.22810678130464876)\n",
        "('F_781', 0.22492197502025818)\n",
        "('F_435', -0.22256213029920702)\n",
        "('F_348', -0.21983841998093082)\n",
        "('F_543', -0.21293371707433595)\n",
        "('F_606', 0.21268027088136643)\n",
        "('F_344', 0.21233433685398501)\n",
        "('F_380', -0.21106312623147758)\n",
        "('F_394', -0.21051089265652617)\n",
        "('F_453', 0.21042289996966618)\n",
        "('F_247', 0.20957088434591453)\n",
        "('F_568', -0.20882548837043421)\n",
        "('F_401', -0.20701728356535495)\n",
        "('F_607', 0.20289533827603415)\n",
        "('F_698', 0.20203704317630924)\n",
        "('F_45', -0.20051180757026291)\n",
        "('F_625', -0.19920709385910509)\n",
        "('F_16', 0.19842891104578952)\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Basic Feature Engineering\n",
      "On the last homework we discussed some text -> numeric feature engineering functions which are commonly used; however, feature engineering is a wide field which is often much simpler.  In particular when you already have all numeric features simple mathematical functions often add significant power to your features.  This helps account for non linear relationships between the features and the target.  \n",
      "\n",
      "Commonly used functions include:\n",
      "\n",
      "1. Log\n",
      "2. Square root\n",
      "3. Squareing\n",
      "4. Sign (1 if the number is positive, 0 otherwise)\n",
      "5. Absolute Value\n",
      "6. Binning\n",
      "\n",
      "Make for each feature (FEATURE) in train_x/test_x create 3 new features: the square of the feature (FEATURE_SQ), the absolute value of the feature (FEATURE_ABS), and the sign of the feature (FEAUTRE_SIGN).  Thus you should have 3200 features in your test_x/train_x data frames."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "\n",
      "# create logger\n",
      "log = logging.getLogger('add_transform_features_logger')\n",
      "log.setLevel(logging.WARN)\n",
      "\n",
      "def add_transform_features(df):\n",
      "    for col in df.columns:\n",
      "        orig_col= df[col]\n",
      "        #\n",
      "        sq_col= np.power(orig_col, 2)\n",
      "        abs_col= np.absolute(orig_col)\n",
      "        sign_col= (~np.signbit(orig_col)).astype(np.float64)\n",
      "        log.info(\"{}, {}, {}\".format(type(orig_col), shape(orig_col), orig_col.head()))\n",
      "        log.info(\"{}, {}, {}\".format(type(sq_col), shape(sq_col), sq_col.head()))\n",
      "        log.info(\"{}, {}, {}\".format(type(abs_col), shape(abs_col), abs_col.head()))\n",
      "        log.info(\"{}, {}, {}\".format(type(sign_col), shape(sign_col), sign_col.head()))\n",
      "        #\n",
      "        df[col + \"_SQ\"]= sq_col\n",
      "        df[col + \"_ABS\"]= abs_col\n",
      "        df[col + \"_SIGN\"]= sign_col"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "add_transform_features(new_train_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_train_x.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>F_0</th>\n",
        "      <th>F_1</th>\n",
        "      <th>F_2</th>\n",
        "      <th>F_3</th>\n",
        "      <th>F_4</th>\n",
        "      <th>F_5</th>\n",
        "      <th>F_6</th>\n",
        "      <th>F_7</th>\n",
        "      <th>F_8</th>\n",
        "      <th>F_9</th>\n",
        "      <th>...</th>\n",
        "      <th>F_796_SIGN</th>\n",
        "      <th>F_797_SQ</th>\n",
        "      <th>F_797_ABS</th>\n",
        "      <th>F_797_SIGN</th>\n",
        "      <th>F_798_SQ</th>\n",
        "      <th>F_798_ABS</th>\n",
        "      <th>F_798_SIGN</th>\n",
        "      <th>F_799_SQ</th>\n",
        "      <th>F_799_ABS</th>\n",
        "      <th>F_799_SIGN</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td> 5.590000e+02</td>\n",
        "      <td>...</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "      <td> 559.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td> 8.262125e-17</td>\n",
        "      <td> 3.495514e-17</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>-1.271096e-17</td>\n",
        "      <td> 1.588870e-18</td>\n",
        "      <td> 1.271096e-17</td>\n",
        "      <td> 2.383305e-17</td>\n",
        "      <td> 1.271096e-17</td>\n",
        "      <td> 5.561046e-18</td>\n",
        "      <td> 6.355481e-18</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.524150</td>\n",
        "      <td>   0.998211</td>\n",
        "      <td>   0.772204</td>\n",
        "      <td>   0.481216</td>\n",
        "      <td>   0.998211</td>\n",
        "      <td>   0.784676</td>\n",
        "      <td>   0.490161</td>\n",
        "      <td>   0.998211</td>\n",
        "      <td>   0.794938</td>\n",
        "      <td>   0.493739</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td> 1.000000e+00</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.499864</td>\n",
        "      <td>   1.564771</td>\n",
        "      <td>   0.634534</td>\n",
        "      <td>   0.500095</td>\n",
        "      <td>   1.561464</td>\n",
        "      <td>   0.619015</td>\n",
        "      <td>   0.500351</td>\n",
        "      <td>   1.405922</td>\n",
        "      <td>   0.605757</td>\n",
        "      <td>   0.500409</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>-1.216883e+00</td>\n",
        "      <td>-2.745079e+00</td>\n",
        "      <td>  -4.596358</td>\n",
        "      <td>-7.881653e+00</td>\n",
        "      <td>-2.525388e+00</td>\n",
        "      <td>-6.003730e+00</td>\n",
        "      <td>-6.085844e+00</td>\n",
        "      <td>-5.170691e+00</td>\n",
        "      <td>-3.625285e+00</td>\n",
        "      <td>-4.728695e+00</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000003</td>\n",
        "      <td>   0.001667</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000016</td>\n",
        "      <td>   0.004034</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000002</td>\n",
        "      <td>   0.001526</td>\n",
        "      <td>   0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>-5.964608e-01</td>\n",
        "      <td>-3.830998e-01</td>\n",
        "      <td>  -0.405811</td>\n",
        "      <td>-5.039266e-01</td>\n",
        "      <td>-4.224896e-01</td>\n",
        "      <td>-2.791551e-01</td>\n",
        "      <td>-2.283447e-01</td>\n",
        "      <td>-3.285434e-01</td>\n",
        "      <td>-5.495358e-01</td>\n",
        "      <td>-4.591213e-01</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.076029</td>\n",
        "      <td>   0.275733</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.087047</td>\n",
        "      <td>   0.295037</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.112321</td>\n",
        "      <td>   0.335143</td>\n",
        "      <td>   0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>-2.650025e-01</td>\n",
        "      <td>-2.619726e-01</td>\n",
        "      <td>  -0.126441</td>\n",
        "      <td>-1.685755e-01</td>\n",
        "      <td>-2.607282e-01</td>\n",
        "      <td>-6.713376e-02</td>\n",
        "      <td> 1.571780e-02</td>\n",
        "      <td> 1.732432e-02</td>\n",
        "      <td>-1.650671e-01</td>\n",
        "      <td> 1.527576e-02</td>\n",
        "      <td>...</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   0.395245</td>\n",
        "      <td>   0.628685</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.450128</td>\n",
        "      <td>   0.670916</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.436379</td>\n",
        "      <td>   0.660590</td>\n",
        "      <td>   0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> 2.024388e-01</td>\n",
        "      <td>-1.971837e-02</td>\n",
        "      <td>   0.013244</td>\n",
        "      <td> 1.667757e-01</td>\n",
        "      <td> 2.245561e-01</td>\n",
        "      <td> 3.569089e-01</td>\n",
        "      <td> 2.597803e-01</td>\n",
        "      <td> 3.631920e-01</td>\n",
        "      <td> 2.194016e-01</td>\n",
        "      <td> 4.896728e-01</td>\n",
        "      <td>...</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.278109</td>\n",
        "      <td>   1.130523</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.173256</td>\n",
        "      <td>   1.083170</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.221919</td>\n",
        "      <td>   1.105397</td>\n",
        "      <td>   1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 4.137444e+00</td>\n",
        "      <td> 4.522549e+00</td>\n",
        "      <td>   5.600640</td>\n",
        "      <td> 5.532395e+00</td>\n",
        "      <td> 7.665582e+00</td>\n",
        "      <td> 5.233399e+00</td>\n",
        "      <td> 4.408843e+00</td>\n",
        "      <td> 5.205340e+00</td>\n",
        "      <td> 1.098453e+01</td>\n",
        "      <td> 5.708041e+00</td>\n",
        "      <td>...</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>  14.110343</td>\n",
        "      <td>   3.756374</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>  14.898214</td>\n",
        "      <td>   3.859820</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   9.464465</td>\n",
        "      <td>   3.076437</td>\n",
        "      <td>   1.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 3200 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "                F_0           F_1         F_2           F_3           F_4  \\\n",
        "count  5.590000e+02  5.590000e+02  559.000000  5.590000e+02  5.590000e+02   \n",
        "mean   8.262125e-17  3.495514e-17    0.000000 -1.271096e-17  1.588870e-18   \n",
        "std    1.000000e+00  1.000000e+00    1.000000  1.000000e+00  1.000000e+00   \n",
        "min   -1.216883e+00 -2.745079e+00   -4.596358 -7.881653e+00 -2.525388e+00   \n",
        "25%   -5.964608e-01 -3.830998e-01   -0.405811 -5.039266e-01 -4.224896e-01   \n",
        "50%   -2.650025e-01 -2.619726e-01   -0.126441 -1.685755e-01 -2.607282e-01   \n",
        "75%    2.024388e-01 -1.971837e-02    0.013244  1.667757e-01  2.245561e-01   \n",
        "max    4.137444e+00  4.522549e+00    5.600640  5.532395e+00  7.665582e+00   \n",
        "\n",
        "                F_5           F_6           F_7           F_8           F_9  \\\n",
        "count  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02   \n",
        "mean   1.271096e-17  2.383305e-17  1.271096e-17  5.561046e-18  6.355481e-18   \n",
        "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
        "min   -6.003730e+00 -6.085844e+00 -5.170691e+00 -3.625285e+00 -4.728695e+00   \n",
        "25%   -2.791551e-01 -2.283447e-01 -3.285434e-01 -5.495358e-01 -4.591213e-01   \n",
        "50%   -6.713376e-02  1.571780e-02  1.732432e-02 -1.650671e-01  1.527576e-02   \n",
        "75%    3.569089e-01  2.597803e-01  3.631920e-01  2.194016e-01  4.896728e-01   \n",
        "max    5.233399e+00  4.408843e+00  5.205340e+00  1.098453e+01  5.708041e+00   \n",
        "\n",
        "           ...       F_796_SIGN    F_797_SQ   F_797_ABS  F_797_SIGN  \\\n",
        "count      ...       559.000000  559.000000  559.000000  559.000000   \n",
        "mean       ...         0.524150    0.998211    0.772204    0.481216   \n",
        "std        ...         0.499864    1.564771    0.634534    0.500095   \n",
        "min        ...         0.000000    0.000003    0.001667    0.000000   \n",
        "25%        ...         0.000000    0.076029    0.275733    0.000000   \n",
        "50%        ...         1.000000    0.395245    0.628685    0.000000   \n",
        "75%        ...         1.000000    1.278109    1.130523    1.000000   \n",
        "max        ...         1.000000   14.110343    3.756374    1.000000   \n",
        "\n",
        "         F_798_SQ   F_798_ABS  F_798_SIGN    F_799_SQ   F_799_ABS  F_799_SIGN  \n",
        "count  559.000000  559.000000  559.000000  559.000000  559.000000  559.000000  \n",
        "mean     0.998211    0.784676    0.490161    0.998211    0.794938    0.493739  \n",
        "std      1.561464    0.619015    0.500351    1.405922    0.605757    0.500409  \n",
        "min      0.000016    0.004034    0.000000    0.000002    0.001526    0.000000  \n",
        "25%      0.087047    0.295037    0.000000    0.112321    0.335143    0.000000  \n",
        "50%      0.450128    0.670916    0.000000    0.436379    0.660590    0.000000  \n",
        "75%      1.173256    1.083170    1.000000    1.221919    1.105397    1.000000  \n",
        "max     14.898214    3.859820    1.000000    9.464465    3.076437    1.000000  \n",
        "\n",
        "[8 rows x 3200 columns]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "add_transform_features(new_test_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_test_x.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>F_0</th>\n",
        "      <th>F_1</th>\n",
        "      <th>F_2</th>\n",
        "      <th>F_3</th>\n",
        "      <th>F_4</th>\n",
        "      <th>F_5</th>\n",
        "      <th>F_6</th>\n",
        "      <th>F_7</th>\n",
        "      <th>F_8</th>\n",
        "      <th>F_9</th>\n",
        "      <th>...</th>\n",
        "      <th>F_796_SIGN</th>\n",
        "      <th>F_797_SQ</th>\n",
        "      <th>F_797_ABS</th>\n",
        "      <th>F_797_SIGN</th>\n",
        "      <th>F_798_SQ</th>\n",
        "      <th>F_798_ABS</th>\n",
        "      <th>F_798_SIGN</th>\n",
        "      <th>F_799_SQ</th>\n",
        "      <th>F_799_ABS</th>\n",
        "      <th>F_799_SIGN</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td>...</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "      <td> 239.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>  -0.048084</td>\n",
        "      <td>   0.077335</td>\n",
        "      <td>   0.020257</td>\n",
        "      <td>   0.072765</td>\n",
        "      <td>   0.021508</td>\n",
        "      <td>  -0.053827</td>\n",
        "      <td>   0.139281</td>\n",
        "      <td>  -0.043456</td>\n",
        "      <td>  -0.033157</td>\n",
        "      <td>   0.011306</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.481172</td>\n",
        "      <td>   1.103632</td>\n",
        "      <td>   0.832741</td>\n",
        "      <td>   0.502092</td>\n",
        "      <td>   0.841250</td>\n",
        "      <td>   0.711797</td>\n",
        "      <td>   0.560669</td>\n",
        "      <td>   0.883814</td>\n",
        "      <td>   0.752080</td>\n",
        "      <td>   0.514644</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>   0.922365</td>\n",
        "      <td>   0.932226</td>\n",
        "      <td>   0.982286</td>\n",
        "      <td>   1.007044</td>\n",
        "      <td>   1.061437</td>\n",
        "      <td>   1.143097</td>\n",
        "      <td>   1.196676</td>\n",
        "      <td>   1.101199</td>\n",
        "      <td>   0.952202</td>\n",
        "      <td>   1.244538</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.500694</td>\n",
        "      <td>   1.672581</td>\n",
        "      <td>   0.641792</td>\n",
        "      <td>   0.501045</td>\n",
        "      <td>   1.290295</td>\n",
        "      <td>   0.579655</td>\n",
        "      <td>   0.497347</td>\n",
        "      <td>   1.197421</td>\n",
        "      <td>   0.565267</td>\n",
        "      <td>   0.500834</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>  -1.080900</td>\n",
        "      <td>  -2.866206</td>\n",
        "      <td>  -3.199509</td>\n",
        "      <td>  -4.863492</td>\n",
        "      <td>  -4.143003</td>\n",
        "      <td>  -7.063837</td>\n",
        "      <td>  -5.109595</td>\n",
        "      <td>  -6.554162</td>\n",
        "      <td>  -4.394223</td>\n",
        "      <td>  -5.677489</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000003</td>\n",
        "      <td>   0.001667</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000016</td>\n",
        "      <td>   0.004034</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.000002</td>\n",
        "      <td>   0.001526</td>\n",
        "      <td>   0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>  -0.587962</td>\n",
        "      <td>  -0.383100</td>\n",
        "      <td>  -0.266126</td>\n",
        "      <td>  -0.168575</td>\n",
        "      <td>  -0.422490</td>\n",
        "      <td>  -0.491176</td>\n",
        "      <td>  -0.228345</td>\n",
        "      <td>  -0.328543</td>\n",
        "      <td>  -0.549536</td>\n",
        "      <td>  -0.459121</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.140803</td>\n",
        "      <td>   0.375174</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.067963</td>\n",
        "      <td>   0.260690</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.075612</td>\n",
        "      <td>   0.274946</td>\n",
        "      <td>   0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>  -0.366990</td>\n",
        "      <td>  -0.201409</td>\n",
        "      <td>  -0.126441</td>\n",
        "      <td>  -0.168575</td>\n",
        "      <td>  -0.098967</td>\n",
        "      <td>  -0.067134</td>\n",
        "      <td>   0.015718</td>\n",
        "      <td>   0.017324</td>\n",
        "      <td>  -0.165067</td>\n",
        "      <td>   0.015276</td>\n",
        "      <td>...</td>\n",
        "      <td>   0.000000</td>\n",
        "      <td>   0.448626</td>\n",
        "      <td>   0.669795</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   0.329378</td>\n",
        "      <td>   0.573915</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   0.369093</td>\n",
        "      <td>   0.607530</td>\n",
        "      <td>   1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>   0.083454</td>\n",
        "      <td>   0.040845</td>\n",
        "      <td>   0.013244</td>\n",
        "      <td>   0.166776</td>\n",
        "      <td>   0.224556</td>\n",
        "      <td>   0.356909</td>\n",
        "      <td>   0.259780</td>\n",
        "      <td>   0.363192</td>\n",
        "      <td>   0.219402</td>\n",
        "      <td>   0.489673</td>\n",
        "      <td>...</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.297416</td>\n",
        "      <td>   1.139042</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.087364</td>\n",
        "      <td>   1.042760</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   1.200591</td>\n",
        "      <td>   1.095700</td>\n",
        "      <td>   1.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>   3.763491</td>\n",
        "      <td>   3.856350</td>\n",
        "      <td>   4.064106</td>\n",
        "      <td>   3.855639</td>\n",
        "      <td>   5.562684</td>\n",
        "      <td>   4.597335</td>\n",
        "      <td>   7.093530</td>\n",
        "      <td>   6.242943</td>\n",
        "      <td>   3.679620</td>\n",
        "      <td>   8.080026</td>\n",
        "      <td>...</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>  10.359455</td>\n",
        "      <td>   3.218611</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   8.004292</td>\n",
        "      <td>   2.829186</td>\n",
        "      <td>   1.000000</td>\n",
        "      <td>   7.262371</td>\n",
        "      <td>   2.694879</td>\n",
        "      <td>   1.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>8 rows \u00d7 3200 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "              F_0         F_1         F_2         F_3         F_4         F_5  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000  239.000000  239.000000   \n",
        "mean    -0.048084    0.077335    0.020257    0.072765    0.021508   -0.053827   \n",
        "std      0.922365    0.932226    0.982286    1.007044    1.061437    1.143097   \n",
        "min     -1.080900   -2.866206   -3.199509   -4.863492   -4.143003   -7.063837   \n",
        "25%     -0.587962   -0.383100   -0.266126   -0.168575   -0.422490   -0.491176   \n",
        "50%     -0.366990   -0.201409   -0.126441   -0.168575   -0.098967   -0.067134   \n",
        "75%      0.083454    0.040845    0.013244    0.166776    0.224556    0.356909   \n",
        "max      3.763491    3.856350    4.064106    3.855639    5.562684    4.597335   \n",
        "\n",
        "              F_6         F_7         F_8         F_9     ...      F_796_SIGN  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000     ...      239.000000   \n",
        "mean     0.139281   -0.043456   -0.033157    0.011306     ...        0.481172   \n",
        "std      1.196676    1.101199    0.952202    1.244538     ...        0.500694   \n",
        "min     -5.109595   -6.554162   -4.394223   -5.677489     ...        0.000000   \n",
        "25%     -0.228345   -0.328543   -0.549536   -0.459121     ...        0.000000   \n",
        "50%      0.015718    0.017324   -0.165067    0.015276     ...        0.000000   \n",
        "75%      0.259780    0.363192    0.219402    0.489673     ...        1.000000   \n",
        "max      7.093530    6.242943    3.679620    8.080026     ...        1.000000   \n",
        "\n",
        "         F_797_SQ   F_797_ABS  F_797_SIGN    F_798_SQ   F_798_ABS  F_798_SIGN  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000  239.000000  239.000000   \n",
        "mean     1.103632    0.832741    0.502092    0.841250    0.711797    0.560669   \n",
        "std      1.672581    0.641792    0.501045    1.290295    0.579655    0.497347   \n",
        "min      0.000003    0.001667    0.000000    0.000016    0.004034    0.000000   \n",
        "25%      0.140803    0.375174    0.000000    0.067963    0.260690    0.000000   \n",
        "50%      0.448626    0.669795    1.000000    0.329378    0.573915    1.000000   \n",
        "75%      1.297416    1.139042    1.000000    1.087364    1.042760    1.000000   \n",
        "max     10.359455    3.218611    1.000000    8.004292    2.829186    1.000000   \n",
        "\n",
        "         F_799_SQ   F_799_ABS  F_799_SIGN  \n",
        "count  239.000000  239.000000  239.000000  \n",
        "mean     0.883814    0.752080    0.514644  \n",
        "std      1.197421    0.565267    0.500834  \n",
        "min      0.000002    0.001526    0.000000  \n",
        "25%      0.075612    0.274946    0.000000  \n",
        "50%      0.369093    0.607530    1.000000  \n",
        "75%      1.200591    1.095700    1.000000  \n",
        "max      7.262371    2.694879    1.000000  \n",
        "\n",
        "[8 rows x 3200 columns]"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###PCA Feature Engineering\n",
      "\n",
      "These simple techniques allow non linear relationships between the features and the target to be captured.  However, one issue is that these features are all univariate and thus our model is unable to capture relationships of multiple variables to the target.  A common technique is to use product/ratio feature (IE F_0*F_1 or F_0/F_1); however, this becomes intractable since this will produced $O(n^2)$ features for $n$ input features.  Domain expertise can give you ideas on which interactions are likely important, but cannot help you find new insights.  \n",
      "\n",
      "One idea is to use unsupervised learning techniques such as PCA to try and capture these interactions.  Find the top 10 prinipal components of train_x and then add the PCA coefficints for train_x/test_x as features PCA_0, PCA_1, ..., PCA_9.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "\n",
      "k= 20\n",
      "\n",
      "# whiten=True as new features have not been normalized\n",
      "m_train_pca = PCA(n_components=k, whiten=True)\n",
      "m_train_pca.fit(new_train_x)\n",
      "\n",
      "X_train_pca= m_train_pca.transform(new_train_x)\n",
      "X_test_pca= m_train_pca.transform(new_test_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange(k):\n",
      "    n_pca_column= \"PCA_{}\".format(i)\n",
      "    new_train_x[n_pca_column]= X_train_pca[:,i]\n",
      "    new_test_x[n_pca_column]= X_test_pca[:,i]\n",
      "    \n",
      "print new_train_x.describe()\n",
      "print new_test_x.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                F_0           F_1         F_2           F_3           F_4  \\\n",
        "count  5.590000e+02  5.590000e+02  559.000000  5.590000e+02  5.590000e+02   \n",
        "mean   8.262125e-17  3.495514e-17    0.000000 -1.271096e-17  1.588870e-18   \n",
        "std    1.000000e+00  1.000000e+00    1.000000  1.000000e+00  1.000000e+00   \n",
        "min   -1.216883e+00 -2.745079e+00   -4.596358 -7.881653e+00 -2.525388e+00   \n",
        "25%   -5.964608e-01 -3.830998e-01   -0.405811 -5.039266e-01 -4.224896e-01   \n",
        "50%   -2.650025e-01 -2.619726e-01   -0.126441 -1.685755e-01 -2.607282e-01   \n",
        "75%    2.024388e-01 -1.971837e-02    0.013244  1.667757e-01  2.245561e-01   \n",
        "max    4.137444e+00  4.522549e+00    5.600640  5.532395e+00  7.665582e+00   \n",
        "\n",
        "                F_5           F_6           F_7           F_8           F_9  \\\n",
        "count  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02   \n",
        "mean   1.271096e-17  2.383305e-17  1.271096e-17  5.561046e-18  6.355481e-18   \n",
        "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
        "min   -6.003730e+00 -6.085844e+00 -5.170691e+00 -3.625285e+00 -4.728695e+00   \n",
        "25%   -2.791551e-01 -2.283447e-01 -3.285434e-01 -5.495358e-01 -4.591213e-01   \n",
        "50%   -6.713376e-02  1.571780e-02  1.732432e-02 -1.650671e-01  1.527576e-02   \n",
        "75%    3.569089e-01  2.597803e-01  3.631920e-01  2.194016e-01  4.896728e-01   \n",
        "max    5.233399e+00  4.408843e+00  5.205340e+00  1.098453e+01  5.708041e+00   \n",
        "\n",
        "           ...             PCA_10        PCA_11        PCA_12        PCA_13  \\\n",
        "count      ...       5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02   \n",
        "mean       ...      -3.177740e-18 -2.065531e-17 -1.271096e-17 -2.542192e-17   \n",
        "std        ...       1.000896e+00  1.000896e+00  1.000896e+00  1.000896e+00   \n",
        "min        ...      -1.037988e+01 -1.096339e+01 -5.258965e+00 -6.615533e+00   \n",
        "25%        ...      -4.208876e-01 -2.849459e-01 -2.751603e-01 -3.199108e-01   \n",
        "50%        ...      -4.857748e-02  4.655427e-02  3.866612e-02 -1.605255e-02   \n",
        "75%        ...       4.043676e-01  2.917763e-01  3.271646e-01  2.959875e-01   \n",
        "max        ...       3.740440e+00  9.102813e+00  1.117396e+01  7.128043e+00   \n",
        "\n",
        "             PCA_14        PCA_15        PCA_16        PCA_17        PCA_18  \\\n",
        "count  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02  5.590000e+02   \n",
        "mean  -2.065531e-17  1.906644e-17 -2.224418e-17 -2.224418e-17  4.448836e-17   \n",
        "std    1.000896e+00  1.000896e+00  1.000896e+00  1.000896e+00  1.000896e+00   \n",
        "min   -7.144983e+00 -7.194309e+00 -5.010804e+00 -6.418295e+00 -5.419022e+00   \n",
        "25%   -5.018411e-01 -3.711884e-01 -6.101714e-01 -3.894231e-01 -4.147700e-01   \n",
        "50%    8.143329e-03  1.877068e-03 -6.884672e-02 -4.193478e-02  2.395502e-02   \n",
        "75%    4.129380e-01  4.376781e-01  5.196919e-01  3.832566e-01  3.724555e-01   \n",
        "max    7.876083e+00  5.099544e+00  5.136714e+00  6.911439e+00  9.189188e+00   \n",
        "\n",
        "             PCA_19  \n",
        "count  5.590000e+02  \n",
        "mean  -1.271096e-17  \n",
        "std    1.000896e+00  \n",
        "min   -7.623667e+00  \n",
        "25%   -3.293914e-01  \n",
        "50%    8.095371e-03  \n",
        "75%    3.563747e-01  \n",
        "max    5.531897e+00  \n",
        "\n",
        "[8 rows x 3220 columns]\n",
        "              F_0         F_1         F_2         F_3         F_4         F_5  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000  239.000000  239.000000   \n",
        "mean    -0.048084    0.077335    0.020257    0.072765    0.021508   -0.053827   \n",
        "std      0.922365    0.932226    0.982286    1.007044    1.061437    1.143097   \n",
        "min     -1.080900   -2.866206   -3.199509   -4.863492   -4.143003   -7.063837   \n",
        "25%     -0.587962   -0.383100   -0.266126   -0.168575   -0.422490   -0.491176   \n",
        "50%     -0.366990   -0.201409   -0.126441   -0.168575   -0.098967   -0.067134   \n",
        "75%      0.083454    0.040845    0.013244    0.166776    0.224556    0.356909   \n",
        "max      3.763491    3.856350    4.064106    3.855639    5.562684    4.597335   \n",
        "\n",
        "              F_6         F_7         F_8         F_9     ...          PCA_10  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000     ...      239.000000   \n",
        "mean     0.139281   -0.043456   -0.033157    0.011306     ...       -0.022885   \n",
        "std      1.196676    1.101199    0.952202    1.244538     ...        0.621494   \n",
        "min     -5.109595   -6.554162   -4.394223   -5.677489     ...       -2.045941   \n",
        "25%     -0.228345   -0.328543   -0.549536   -0.459121     ...       -0.367032   \n",
        "50%      0.015718    0.017324   -0.165067    0.015276     ...       -0.093171   \n",
        "75%      0.259780    0.363192    0.219402    0.489673     ...        0.296001   \n",
        "max      7.093530    6.242943    3.679620    8.080026     ...        3.080458   \n",
        "\n",
        "           PCA_11      PCA_12      PCA_13      PCA_14      PCA_15      PCA_16  \\\n",
        "count  239.000000  239.000000  239.000000  239.000000  239.000000  239.000000   \n",
        "mean     0.066431   -0.037342    0.063390   -0.080951    0.117120   -0.012700   \n",
        "std      0.693402    0.607700    0.892811    0.790228    0.682513    0.632094   \n",
        "min     -2.163567   -2.594215   -6.568922   -4.787804   -1.828666   -1.394555   \n",
        "25%     -0.205510   -0.196519   -0.237014   -0.279252   -0.228688   -0.439388   \n",
        "50%      0.040565    0.025730    0.051992   -0.026697    0.051186   -0.071289   \n",
        "75%      0.262082    0.253037    0.315170    0.263232    0.409468    0.382620   \n",
        "max      5.805756    3.882290    4.636999    2.912032    3.526258    2.042341   \n",
        "\n",
        "           PCA_17      PCA_18      PCA_19  \n",
        "count  239.000000  239.000000  239.000000  \n",
        "mean     0.029861    0.049730    0.022043  \n",
        "std      0.918503    0.727969    0.593130  \n",
        "min     -4.375669   -1.965967   -2.307315  \n",
        "25%     -0.361322   -0.295217   -0.225248  \n",
        "50%     -0.084410   -0.005946    0.081374  \n",
        "75%      0.268529    0.352219    0.315826  \n",
        "max      4.832470    4.247949    1.732090  \n",
        "\n",
        "[8 rows x 3220 columns]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Feature Selection\n",
      "\n",
      "Earlier, we built a model with the top 10 features from our whole model.  This is a possible way of doing [feature selection](http://en.wikipedia.org/wiki/Feature_selection), or choosing a subset of features to build our model with.  However, this is not the best way for a variety of reasons which you'll come to understand in future classes.  \n",
      "\n",
      "One better method of feature which is easy to understand is [forward selection](http://en.wikipedia.org/wiki/Stepwise_regression).  In this process you start with a model with no features and then search for the single best feature to add.  After you've found it you keep searching for the next feature to add until some stopping criteria is reached.  In this part we will guide you through the process of writing such an algorithm.\n",
      "\n",
      "As a first step, write code which will find the best 1 feature model.  In otherwords, for each feature train a model and evaluate its acuracy on the test data.  Store the best model, the feature used, and its accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def execute_model(t_train_x, s_train_label, t_test_x, m_actual_array):\n",
      "    #Train model/evaluate performance here\n",
      "    new_m_l1_LR= new_logistic_regression()\n",
      "    new_m_l1_LR.fit(t_train_x, s_train_label)\n",
      "\n",
      "    m_predicted= new_m_l1_LR.predict(t_test_x)\n",
      "    result= get_accuracy(m_actual_array, m_predicted)\n",
      "    return result\n",
      "\n",
      "def select_best_feature(feature_names, test_columns, train_x, train_y, test_x, test_y):\n",
      "    m_best_prediction= -1.\n",
      "    m_best_feature= None\n",
      "    #\n",
      "    s_train_label= train_y[\"label\"]\n",
      "    s_test_arr= test_y[\"label\"].values\n",
      "    #\n",
      "    for c in test_columns:\n",
      "        try:\n",
      "            feature_names.append(c)\n",
      "            c_prediction= execute_model(new_train_x[feature_names], s_train_label, new_test_x[feature_names], s_test_arr)\n",
      "            if (c_prediction > m_best_prediction):\n",
      "                m_best_feature= c\n",
      "                m_best_prediction= c_prediction\n",
      "        finally:\n",
      "            feature_names.pop()\n",
      "    if m_best_feature is None:\n",
      "        raise RuntimeError(\"m_best_feature should not be null\")\n",
      "    feature_names.append(m_best_feature)\n",
      "    test_columns.remove(m_best_feature)\n",
      "    return (m_best_feature, m_best_prediction)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "\n",
      "# create logger\n",
      "log = logging.getLogger('feature_selection_logger')\n",
      "log.setLevel(logging.WARN)\n",
      "\n",
      "feature_names= []\n",
      "test_columns= new_train_x.columns.values.tolist()\n",
      "#test_columns=[\"F_183\", \"F_19_SQ\"]\n",
      "select_best_feature(feature_names, test_columns, new_train_x, train_y, new_test_x, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "('F_240', 0.761506)"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that you have figured out how to do a single iteration of forward selection (going from 0 -> 1 feature), use the below boiler plate code to complete the algorithm.\n",
      "\n",
      "Since we want to find the best model with 20 features, we loop through feature selection 20 times.  Add the following steps:\n",
      "\n",
      "1. Train models using the features in $f$ + 1 additional feature\n",
      "2. Find the additional feature that results in the highest accuracy\n",
      "3. Add the best addditional feature to $f$ and the accuracy of the best model to $acc$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "\n",
      "# create logger\n",
      "log = logging.getLogger('multiple_feature_selection_logger')\n",
      "log.setLevel(logging.WARN)\n",
      "\n",
      "acc = []\n",
      "features  = []\n",
      "\n",
      "k= 60\n",
      "feature_names= []\n",
      "test_columns= new_train_x.columns.values.tolist()\n",
      "#test_columns= [\"F_183\", \"F_19_SQ\"]\n",
      "for i in xrange(k):\n",
      "    #TODO: search for the best feature to add to features to maximize performance\n",
      "    #TODO: add the best feature to features\n",
      "    #TODO: add the accuracy of the best model to acc\n",
      "    log.warn(\"Selecting feature {}\".format(i+1))\n",
      "    m_result= select_best_feature(feature_names, test_columns, new_train_x, train_y, new_test_x, test_y)\n",
      "    m_feature_name= m_result[0]\n",
      "    m_feature_accuracy= m_result[1]\n",
      "    features.append(m_feature_name)\n",
      "    acc.append(m_feature_accuracy)\n",
      "    log.warn(\"Selected feature={}; cumulative Accuracy={}\".format(m_feature_name, m_feature_accuracy))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 1\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_240; cumulative Accuracy=0.761506\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_368_SQ; cumulative Accuracy=0.799163\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 3\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_396; cumulative Accuracy=0.832636\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 4\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_169_ABS; cumulative Accuracy=0.853556\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 5\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_348; cumulative Accuracy=0.866109\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 6\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_711_ABS; cumulative Accuracy=0.878661\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 7\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_315; cumulative Accuracy=0.887029\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 8\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_178_SQ; cumulative Accuracy=0.891213\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 9\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_628_SQ; cumulative Accuracy=0.899582\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 10\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_79; cumulative Accuracy=0.903766\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 11\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_244; cumulative Accuracy=0.90795\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 12\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_77_SQ; cumulative Accuracy=0.912134\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 13\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_284_ABS; cumulative Accuracy=0.916318\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 14\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_393_SQ; cumulative Accuracy=0.920502\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 15\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_1; cumulative Accuracy=0.920502\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 16\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_2; cumulative Accuracy=0.920502\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 17\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_5; cumulative Accuracy=0.920502\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 18\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_7; cumulative Accuracy=0.920502\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 19\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_8; cumulative Accuracy=0.920502\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 20\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_0; cumulative Accuracy=0.920502\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 21\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_603; cumulative Accuracy=0.92887\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 22\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_136; cumulative Accuracy=0.933054\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 23\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_24_SQ; cumulative Accuracy=0.937238\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 24\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_4; cumulative Accuracy=0.941423\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 25\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_9; cumulative Accuracy=0.941423\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 26\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_10; cumulative Accuracy=0.941423\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 27\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_752_SQ; cumulative Accuracy=0.945607\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 28\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_11; cumulative Accuracy=0.945607\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 29\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_270; cumulative Accuracy=0.949791\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 30\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_60_SQ; cumulative Accuracy=0.953975\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 31\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_318_SQ; cumulative Accuracy=0.958159\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 32\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_267_SIGN; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 33\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_12; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 34\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_15; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 35\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_19; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 36\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_25; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_23; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 38\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_21; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 39\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_34; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 40\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_38; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 41\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_39; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 42\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_42; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 43\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_44; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 44\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_54; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 45\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_24; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 46\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_49; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 47\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_63; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 48\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_43; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 49\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_66; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 50\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_67; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 51\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_68; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 52\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_73; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 53\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_61; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 54\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_74; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 55\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_83; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 56\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_85; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 57\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_26; cumulative Accuracy=0.962343\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 58\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_113_ABS; cumulative Accuracy=0.966527\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 59\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_84; cumulative Accuracy=0.966527\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selecting feature 60\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:multiple_feature_selection_logger:Selected feature=F_104; cumulative Accuracy=0.966527\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now plot the accuracy as a function of the number of features.  Also plot a horizontal line showing the accuracy of the baseline model and the original 20 high coefficient feature model.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def doPlot():\n",
      "    fig, ax= plt.subplots()\n",
      "    plt.grid()\n",
      "    plt.xlabel('# of Features')\n",
      "    plt.ylabel('Accuracy')\n",
      "    try:\n",
      "        n = len(features)\n",
      "        l_x= range(n)\n",
      "        ax.plot(l_x, acc, color='g', label='forward_selection')\n",
      "        plt.axhline(acc_baseline, color='b', linestyle='--', label='baseline')\n",
      "        plt.axhline(acc_with_best, color='r', linestyle='--', label='20 high coefficient')\n",
      "        #\n",
      "        ax.legend(fontsize='small', loc=\"best\")\n",
      "        #fig.savefig(\"test.png\", dpi=125)\n",
      "    finally:\n",
      "        plt.show()\n",
      "\n",
      "doPlot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f6a105753d0>"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "global name 'features' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-9e3cdf607a28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mdoPlot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-1-9e3cdf607a28>\u001b[0m in \u001b[0;36mdoPlot\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0ml_x\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'forward_selection'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: global name 'features' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Discussion of Results\n",
      "Now discuss the results.  In particular address the following issues:\n",
      "\n",
      "1. With the feature engineering and forward selection is accuracy better than baseline attainable?  If so how many features are required to beat the baseline?\n",
      "2. Based on the trajectory of the accuracy plot, do you think more features will significantly improve accuracy?\n",
      "3. Do some of the most important features from the baseline model still show up in our final forward selection model?  Are they transformed?\n",
      "4. Do all 3 sets of features (raw, transformed, pca) appear in the final model?  Which set seems to be the most important?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#A\n",
      "Afer execution of the 3 separate models, feature engineering w/ forward selection is the most optimal, with a maximum of 0.966527 when 58 features selected. Feature engineering w/ forward selection achieves better accuracy than the baseline after 14 features are selected.\n",
      "\n",
      "Plotting the results of the experiment, it appears that the initial 32 features selected result in drastic improvement in accuracy. Based on the trajectory of the forward selection curve, additional features selected (after 32) will not significantly improve the accuracy of the method.\n",
      "\n",
      "There are some important features from the baseline model that show up in the final forward selection:\n",
      "\n",
      " - F_240\n",
      " - F_396\n",
      " - F_348\n",
      " - F_393_SQ\n",
      " \n",
      "And one is transformed, however, the number of important features from the baseline model that are chosen in feature engineering w/ forward selection is minimal (about 0.07)\n",
      "\n",
      "Assuming iteration over the DataFrame columns from start to end, and the columns within the Data Frame are structured as\n",
      "    - 800 normalized\n",
      "    - 2400 transformed (not normalized)\n",
      "    - 20 PCA (constructed with whiten=True)\n",
      "Only raw and transformed features appear in the final model. Assessing the selected features during forward selection, it is seen that the number of raw features chosen are greater than that of transformed. In addition, the percentage of raw features that are selected in the first 20 is 0.6 vs 0.4 for transformed features. Therefore, the set that seems most important in feature engineering w/ forward selection is the raw feature set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def strip_suffix(s):\n",
      "    suffixes= [\"_SQ\", \"_ABS\", \"_SIGN\"]\n",
      "    for suffix in suffixes:\n",
      "        i= s.find(suffix)\n",
      "        if i >= 0:\n",
      "            return s[0:i]\n",
      "    return s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Text= \"\"\"\n",
      "WARNING:20_feature_selection_logger:Selecting feature 1\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_240; cumulative Accuracy=0.761506\n",
      "WARNING:20_feature_selection_logger:Selecting feature 2\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_368_SQ; cumulative Accuracy=0.799163\n",
      "WARNING:20_feature_selection_logger:Selecting feature 3\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_396; cumulative Accuracy=0.832636\n",
      "WARNING:20_feature_selection_logger:Selecting feature 4\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_169_ABS; cumulative Accuracy=0.853556\n",
      "WARNING:20_feature_selection_logger:Selecting feature 5\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_348; cumulative Accuracy=0.866109\n",
      "WARNING:20_feature_selection_logger:Selecting feature 6\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_711_ABS; cumulative Accuracy=0.878661\n",
      "WARNING:20_feature_selection_logger:Selecting feature 7\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_315; cumulative Accuracy=0.887029\n",
      "WARNING:20_feature_selection_logger:Selecting feature 8\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_178_SQ; cumulative Accuracy=0.891213\n",
      "WARNING:20_feature_selection_logger:Selecting feature 9\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_628_SQ; cumulative Accuracy=0.899582\n",
      "WARNING:20_feature_selection_logger:Selecting feature 10\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_79; cumulative Accuracy=0.903766\n",
      "WARNING:20_feature_selection_logger:Selecting feature 11\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_244; cumulative Accuracy=0.90795\n",
      "WARNING:20_feature_selection_logger:Selecting feature 12\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_77_SQ; cumulative Accuracy=0.912134\n",
      "WARNING:20_feature_selection_logger:Selecting feature 13\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_284_ABS; cumulative Accuracy=0.916318\n",
      "WARNING:20_feature_selection_logger:Selecting feature 14\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_393_SQ; cumulative Accuracy=0.920502\n",
      "WARNING:20_feature_selection_logger:Selecting feature 15\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_1; cumulative Accuracy=0.920502\n",
      "WARNING:20_feature_selection_logger:Selecting feature 16\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_2; cumulative Accuracy=0.920502\n",
      "WARNING:20_feature_selection_logger:Selecting feature 17\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_5; cumulative Accuracy=0.920502\n",
      "WARNING:20_feature_selection_logger:Selecting feature 18\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_7; cumulative Accuracy=0.920502\n",
      "WARNING:20_feature_selection_logger:Selecting feature 19\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_8; cumulative Accuracy=0.920502\n",
      "WARNING:20_feature_selection_logger:Selecting feature 20\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_0; cumulative Accuracy=0.920502\n",
      "WARNING:20_feature_selection_logger:Selecting feature 21\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_603; cumulative Accuracy=0.92887\n",
      "WARNING:20_feature_selection_logger:Selecting feature 22\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_136; cumulative Accuracy=0.933054\n",
      "WARNING:20_feature_selection_logger:Selecting feature 23\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_24_SQ; cumulative Accuracy=0.937238\n",
      "WARNING:20_feature_selection_logger:Selecting feature 24\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_4; cumulative Accuracy=0.941423\n",
      "WARNING:20_feature_selection_logger:Selecting feature 25\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_9; cumulative Accuracy=0.941423\n",
      "WARNING:20_feature_selection_logger:Selecting feature 26\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_10; cumulative Accuracy=0.941423\n",
      "WARNING:20_feature_selection_logger:Selecting feature 27\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_752_SQ; cumulative Accuracy=0.945607\n",
      "WARNING:20_feature_selection_logger:Selecting feature 28\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_11; cumulative Accuracy=0.945607\n",
      "WARNING:20_feature_selection_logger:Selecting feature 29\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_270; cumulative Accuracy=0.949791\n",
      "WARNING:20_feature_selection_logger:Selecting feature 30\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_60_SQ; cumulative Accuracy=0.953975\n",
      "WARNING:20_feature_selection_logger:Selecting feature 31\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_318_SQ; cumulative Accuracy=0.958159\n",
      "WARNING:20_feature_selection_logger:Selecting feature 32\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_267_SIGN; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 33\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_12; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 34\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_15; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 35\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_19; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 36\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_25; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 37\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_23; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 38\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_21; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 39\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_34; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 40\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_38; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 41\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_39; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 42\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_42; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 43\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_44; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 44\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_54; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 45\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_24; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 46\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_49; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 47\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_63; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 48\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_43; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 49\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_66; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 50\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_67; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 51\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_68; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 52\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_73; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 53\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_61; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 54\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_74; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 55\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_83; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 56\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_85; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 57\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_26; cumulative Accuracy=0.962343\n",
      "WARNING:20_feature_selection_logger:Selecting feature 58\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_113_ABS; cumulative Accuracy=0.966527\n",
      "WARNING:20_feature_selection_logger:Selecting feature 59\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_84; cumulative Accuracy=0.966527\n",
      "WARNING:20_feature_selection_logger:Selecting feature 60\n",
      "WARNING:20_feature_selection_logger:Selected feature=F_104; cumulative Accuracy=0.966527\n",
      "\"\"\"\n",
      "\n",
      "for c in \"\\t \":\n",
      "    Text= Text.replace(c, '')\n",
      "\n",
      "f_result= []\n",
      "s_arr= Text.split(\"\\n\")\n",
      "s_search_for= \"Selectedfeature=\"\n",
      "s_len= len(s_search_for)\n",
      "for e in s_arr:\n",
      "    if len(e) == 0:\n",
      "        continue\n",
      "    m_i= e.find(s_search_for)\n",
      "    if m_i >= 0:\n",
      "        i_start= m_i + s_len\n",
      "        i_end= e.find(\";\")\n",
      "        n_string= e[i_start:i_end]\n",
      "        e_i= e.rindex(\"=\") + 1\n",
      "        c_accuracy= float(e[e_i:])\n",
      "        f_result.append((n_string, c_accuracy))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Determine importance of coefficients\n",
      "\n",
      "m_coef_set= set([Coef_List[i][0] for i in xrange(len(f_result))])\n",
      "\n",
      "m_raw_i= 0\n",
      "m_trans_i= 0\n",
      "m_no_match= 0\n",
      "for e in f_result:\n",
      "    s= e[0]\n",
      "    s_test= strip_suffix(s)\n",
      "    if s_test in m_coef_set:\n",
      "        if s in m_coef_set:\n",
      "            m_raw_i+=1\n",
      "        else:\n",
      "            m_trans_i+=1\n",
      "    else:\n",
      "        m_no_match+=1\n",
      "\n",
      "f_n= float(len(f_result))\n",
      "m_perc_raw= round(float(m_raw_i)/f_n, 6)\n",
      "m_perc_trans= round(float(m_trans_i)/f_n, 6)\n",
      "m_perc_not_found= round(float(m_no_match)/f_n, 6)\n",
      "\n",
      "print \"Raw={}, Transformed= {}, No Coefficient Match= {}\".format(m_perc_raw,m_perc_trans,m_perc_not_found)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Raw=0.05, Transformed= 0.016667, No Coefficient Match= 0.933333\n"
       ]
      }
     ],
     "prompt_number": 27
    }
   ],
   "metadata": {}
  }
 ]
}